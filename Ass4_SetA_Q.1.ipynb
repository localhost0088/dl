{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#set A que 1\n",
        "#Create a simple RNN model for generating data using text and predict the text using a pre-trained model.(Use ReLU and Softmax activation function). (Use shakespeare.txt file)\n",
        "# --- Step 1: Imports ---\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Activation\n",
        "\n",
        "# --- Step 2: Download dataset if not available ---\n",
        "file_path = \"shakespeare.txt\"\n",
        "if not os.path.exists(file_path):\n",
        "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "    response = requests.get(url)\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(response.text)\n",
        "    print(\"✅ Dataset downloaded!\")\n",
        "else:\n",
        "    print(\"📂 Dataset already exists.\")\n",
        "\n",
        "# --- Step 3: Load text data ---\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f\"Dataset length: {len(text)} characters\")\n",
        "\n",
        "# --- Step 4: Tokenization ---\n",
        "vocab = sorted(set(text))\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocabulary size:\", vocab_size)\n",
        "\n",
        "char_to_idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx_to_char = np.array(vocab)\n",
        "\n",
        "# Convert entire text into integers\n",
        "text_as_int = np.array([char_to_idx[c] for c in text])\n",
        "\n",
        "# --- Step 5: Create training sequences ---\n",
        "seq_length = 40\n",
        "examples_per_epoch = len(text_as_int) // (seq_length + 1)\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "# Create sequences of length seq_length+1\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]   # first 40 chars\n",
        "    target_text = chunk[1:]   # next 40 chars\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Shuffle and batch\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# --- Step 6: Build model ---\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=64),\n",
        "    SimpleRNN(128, activation='tanh', return_sequences=True),  # 🔑 fix\n",
        "    Dense(vocab_size),\n",
        "    Activation('softmax')\n",
        "])\n",
        "\n",
        "# Build explicitly\n",
        "model.build(input_shape=(None, seq_length))\n",
        "\n",
        "# Compile\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# --- Step 7: Train ---\n",
        "EPOCHS = 5\n",
        "history = model.fit(dataset, epochs=EPOCHS)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# --- Step 8: Text Generation Function ---\n",
        "def generate_text(model, seed_text, char_to_int, idx_to_char, length=200): # Changed int_to_char to idx_to_char\n",
        "    generated = seed_text\n",
        "    for _ in range(length):\n",
        "        # Encode seed text → integers\n",
        "        encoded = np.array([[char_to_int.get(c, 0) for c in seed_text]])\n",
        "\n",
        "        # Predict next char probs for all timesteps → take last one\n",
        "        preds = model.predict(encoded, verbose=0)\n",
        "        preds = preds[:, -1, :]   # last timestep only\n",
        "\n",
        "        # Sample from probability distribution (better than argmax)\n",
        "        next_index = np.random.choice(len(preds[0]), p=preds[0])\n",
        "        next_char = idx_to_char[next_index] # Changed int_to_char to idx_to_char\n",
        "\n",
        "        # Append & shift seed\n",
        "        generated += next_char\n",
        "        seed_text = seed_text[1:] + next_char\n",
        "    return generated\n",
        "\n",
        "\n",
        "# --- Step 9: Try it out ---\n",
        "# Make sure to run the previous cell first to define 'text', 'seq_length', 'char_to_idx', and 'idx_to_char'\n",
        "seed = text[:seq_length]   # first 40 chars as seed\n",
        "print(\"Seed:\\n\", seed)\n",
        "print(\"\\nGenerated Text:\\n\")\n",
        "print(generate_text(model, seed, char_to_idx, idx_to_char, length=300)) # Changed int_to_char to idx_to_char\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 846
        },
        "id": "RYoiJLq-Zo9z",
        "outputId": "1d848bc9-2dc1-4216-f411-8af878ba3156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset downloaded!\n",
            "Dataset length: 1115394 characters\n",
            "Vocabulary size: 65\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m4,160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m24,704\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m65\u001b[0m)         │         \u001b[38;5;34m8,385\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m65\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,385</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m37,249\u001b[0m (145.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">37,249</span> (145.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m37,249\u001b[0m (145.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">37,249</span> (145.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m425/425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 32ms/step - accuracy: 0.2517 - loss: 2.8595\n",
            "Epoch 2/5\n",
            "\u001b[1m425/425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 32ms/step - accuracy: 0.4015 - loss: 2.0820\n",
            "Epoch 3/5\n",
            "\u001b[1m425/425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.4370 - loss: 1.9314\n",
            "Epoch 4/5\n",
            "\u001b[1m425/425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 33ms/step - accuracy: 0.4604 - loss: 1.8470\n",
            "Epoch 5/5\n",
            "\u001b[1m425/425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - accuracy: 0.4746 - loss: 1.7912\n",
            "Seed:\n",
            " First Citizen:\n",
            "Before we proceed any fur\n",
            "\n",
            "Generated Text:\n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any furrevatuse himy centies be exice,\n",
            "She's wild like hast cound! I shook knoe\n",
            "jequagertter\n",
            "As to to shall, whop kree\n",
            "Marrapam to more our thene than what ammonter besforied, to the my grothing of my charstion and wish the cool.\n",
            "Why, I canse?\n",
            "\n",
            "SABIO:\n",
            "Ye\n",
            "would Lefort not,\n",
            "For is lor! Which ip dought wigh i\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}